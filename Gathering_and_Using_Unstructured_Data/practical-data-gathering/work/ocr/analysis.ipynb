{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "* In the timeframe it is unfeasible to run through a real-world document pipeline for classification...\n",
    "* ...however we can run through the same process for a simple test case\n",
    "\n",
    "## Problem statement\n",
    "* Imagine you have access to a subset of this raw text dataset https://www.kaggle.com/c/spooky-author-identification.\n",
    "* Your boss then sends you a folder containing a bunch (1000's) of scanned pages from books that somehow exploded!\n",
    "    * They say they need them sorted by author ASAP and your job depends on it!\n",
    "    * We're lucky though because the scan quality is really good (so no cleaning required)\n",
    "    * What do you do?!!\n",
    "\n",
    "Don't worry we've got a solution in mind...\n",
    "\n",
    "## Proposed solution\n",
    "\n",
    "* Train a text classifier on the raw text data (training set)\n",
    "* OCR all the given images and create another raw text dataset\n",
    "* Apply your classifier to the OCR'ed text and boom! you have a sorted set (up to the accuracy of the classifier)\n",
    "* Job Saved!\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train text classifier\n",
    "* How do we represent text as an input to a mathematical model?\n",
    "    * This question has many answers but the simplest would be counting!\n",
    "    * We can count occurences of words across a corpus (collection of documents)\n",
    "    * We can then map a document (text string) to a vector of length $N$ where $N$ is the size of the vocabulary!\n",
    "* Okay but how do we actually do this?\n",
    "    * Could write your own algorithms, but time is of the essence so we should take to the internet.\n",
    "    * The resounding answer is: sci-kit learn\n",
    "    \n",
    "## Sci-kit learn\n",
    "scikit learn is an amazing open source library with a ton of robust implemenations of your favorite ML models:\n",
    "https://scikit-learn.org/stable/\n",
    "\n",
    "* In particular we will be taking advantage of a few modules:\n",
    "    * text feature extraction https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text\n",
    "    * pipeline https://scikit-learn.org/stable/modules/classes.html#module-sklearn.pipeline (for \"productionalizing\")\n",
    "    * multinomial naive bayes https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "    * random forest https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "    \n",
    "## Loose steps\n",
    "\n",
    "Typically in a realworld scenario data cleaning and gathering would be your number 1 (I can't stress this enough) issue. \n",
    "\n",
    "But we have a nice dataset in this case so we can follow a few steps to develop a robust classifier:\n",
    "\n",
    "## NUMBER ONE RULE: This is an experiment, treat it as such with scientific rigor and reasoning\n",
    "\n",
    "1) Define your metric and your goal with that metric (accuracy, f1 score, recall, precision etc..) \n",
    "\n",
    "2) Define your training set and testing set (These need to be distinct with no overlap!)\n",
    "\n",
    "3) Define your preprocessing steps, we need to convert our raw data into a usable form by a mathematical model\n",
    "\n",
    "4) Select a set of candidate algorithms (we'll test them all to pick a winner)\n",
    "\n",
    "5) Define your validation strategy (how do we decide one model is better than another BEFORE THE TEST SET)\n",
    "\n",
    "6) Setup your scripts and follow your procedure\n",
    "\n",
    "7) Select the best candidate and measure the chosen metric on your test set to see if it's acceptable, if not go back and tweak your procedure and repeat.\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import pytesseract\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab our Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('images/train.csv')\n",
    "\n",
    "\n",
    "print('Class Breakdown:')\n",
    "print(train['author'].value_counts())\n",
    "print(f'Total:{len(train)}')\n",
    "print('\\n')\n",
    "\n",
    "print('Sample:')\n",
    "for row in train.head(5).iterrows():\n",
    "    data = row[1]\n",
    "    \n",
    "    print(data.author)\n",
    "    print(data.text)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = train.text.tolist()\n",
    "labels_train = train.author.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "#vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "#rf = RandomForestClassifier()\n",
    "\n",
    "#ada = AdaBoostClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('vectorizer',vectorizer),('base_clf',mnb)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup 5-Fold Cross-Validation for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'vectorizer__analyzer':['word'],\n",
    "              'vectorizer__ngram_range':[(1,1)]}\n",
    "\n",
    "clf = GridSearchCV(pipeline,\n",
    "                   param_grid=param_grid,\n",
    "                   refit=True,\n",
    "                   verbose=2,\n",
    "                   cv=5)\n",
    "\n",
    "\n",
    "clf.fit(text_train,labels_train)\n",
    "\n",
    "results = pd.DataFrame(data = clf.cv_results_)\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR test set sample (a larger set would take too long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"path\":[],\n",
    "        \"id\":[],\n",
    "        \"predict\":[],\n",
    "        \"text\":[]\n",
    "       }\n",
    "\n",
    "directory = 'images/test-set-sample'\n",
    "\n",
    "filenames = [name for name in os.listdir(directory) if name.endswith('.png')]\n",
    "\n",
    "for fn in filenames:\n",
    "\n",
    "    print(f'Processing {fn}')\n",
    "\n",
    "    full_path = os.path.join(directory,fn)\n",
    "    pilimg = Image.open(full_path)\n",
    "\n",
    "    img_id = fn.split('.')[0]\n",
    "    \n",
    "    # Get orientation first, gives tesseract a better chance at extraction.\n",
    "    try:\n",
    "        orientation_results = pytesseract.image_to_osd(pilimg,output_type='dict')\n",
    "        degrees = orientation_results['rotate']\n",
    "        if degrees != 0:\n",
    "            pilimg = pilimg.rotate(-degrees,expand=True)\n",
    "            \n",
    "        text = pytesseract.image_to_string(pilimg,lang='eng')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        text = ''\n",
    "    \n",
    "        \n",
    "    data['predict'].append(clf.predict([text])[0])\n",
    "    data['path'].append(full_path)\n",
    "    data['id'].append(img_id)\n",
    "    data['text'].append(text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot Check Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "test = pd.read_csv('images/test.csv')\n",
    "\n",
    "sample = pd.DataFrame(data=data).merge(test[['id','author']],on='id',how='left')\n",
    "\n",
    "for row in sample.iterrows():\n",
    "    d = row[1]\n",
    "    \n",
    "    img = Image.open(d.path)\n",
    "    \n",
    "    display(img)\n",
    "    \n",
    "    print('Extracted Text:')\n",
    "    print(d.text)\n",
    "    print('\\n')\n",
    "    \n",
    "    print(f'Predicted: {d.predict}, Actual: {d.author}')\n",
    "    \n",
    "    cont = input('continue? (y or n)')\n",
    "    \n",
    "    if cont not in ['y','']:\n",
    "        break\n",
    "    else:\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full test set\n",
    "Imagine we OCR'ed everything! we'll check our performance on the full hold-out (test) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = test['text'].tolist()\n",
    "labels_test = test['author'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "preds = clf.predict(text_test)\n",
    "\n",
    "fig_labels = clf.classes_\n",
    "CM=confusion_matrix(labels_test,preds,labels=fig_labels)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(20,20))\n",
    "\n",
    "sns.heatmap(CM,annot=True,xticklabels=fig_labels, yticklabels=fig_labels ,ax = ax)\n",
    "ax.set_title(\"Overall Accuracy:{}\".format(accuracy_score(preds,labels_test)))\n",
    "ax.set_xlabel('Predicted label')\n",
    "ax.set_ylabel('True label')\n",
    "fig.set_facecolor('w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "* Can you beat this base accuracy? \n",
    "* Can you beat the best on kaggle? (note they use a different metric there)\n",
    "* Is it possible to classify these images based on their visual content? (as impractical as that may be).\n",
    "* Can you think of other features you could generate?\n",
    "* What about other modelling strategies (sequence models?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
